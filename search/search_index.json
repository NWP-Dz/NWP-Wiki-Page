{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ONM-NWP Documentation","text":"<pre><code>                       Welcome to the ONM-NWP documentation\n</code></pre>"},{"location":"#about-us","title":"About Us","text":"<p>This Wiki Page contains all the activities and components  of the NWP departement of the National Office of Meteorology. The main point of Creating it is to help the newcomers to adapt quickly to the atmospher of work and help them understand the basics of NWP. If you have any questions about any topic contact the the Members of the NWP departement and they will guide you to achieve and realise the same tasks mentionned in this wiki page. </p>"},{"location":"about/","title":"About Us","text":"<p>This Wiki Page contains all the activities and components  of the NWP departement of the National Office of Meteorology. The main point of Creating it is to help the newcomers to adapt quickly to the atmospher of work and help them understand the basics of NWP. If you have any questions about any topic contact the the Members of the NWP departement and they will guide you to achieve and realise the same tasks mentionned in this wiki page.</p>"},{"location":"contact/","title":"Contact","text":""},{"location":"contact/#meet-the-team","title":"Meet the Team","text":""},{"location":"contact/#walid-chikhi","title":"Walid Chikhi","text":"<p>GitHub: walidchikhi</p>"},{"location":"contact/#ayoub-mehbali","title":"Ayoub Mehbali","text":"<p>GitHub: mhbayoub</p>"},{"location":"contact/#imad-eddine-helali","title":"Imad Eddine Helali","text":"<p>GitHub: imadhelali</p>"},{"location":"contact/#nour-el-islam-kerroumi","title":"Nour El Islam Kerroumi","text":"<p>GitHub: KERROUMI</p>"},{"location":"contact/#issam-lagha","title":"Issam Lagha","text":"<p>GitHub: issamlagha</p>"},{"location":"contact/#amine-khedim","title":"Amine Khedim","text":"<p>GitHub: aminekhedim</p>"},{"location":"contact/#mohand-ait-meziane","title":"Mohand Ait Meziane","text":"<p>GitHub: aitmezianeMO</p>"},{"location":"contact/#bahlouli-abdelhake","title":"Bahlouli Abdelhake","text":"<p>GitHub: </p>"},{"location":"contact/#sofiane-kameche","title":"Sofiane Kameche","text":"<p>GitHub: KamecheMohamedSofiane</p>"},{"location":"contact/#nadia-aber","title":"Nadia Aber","text":"<p>GitHub: Nadiaaber</p>"},{"location":"contact/#chahrazed-bouzerma","title":"Chahrazed Bouzerma","text":"<p>GitHub: cbouzerma</p>"},{"location":"Control_%26_Verification/HARP/","title":"harp","text":"<p>harp is a set of R packages for manipulation, analysis, visualization and verification of data from regular grids. The most up to date  information and tutorials can be found on the website for the 2024 training course</p>"},{"location":"Cycles/cycle/","title":"This Section is Under Construction \ud83d\udea7","text":"<p>We are currently working on this section. Please check back later for updates!</p> <p></p> <p>Thank you for your patience.</p>"},{"location":"ForecastModel/Atmospheric/ALADIN/","title":"Forecast Model","text":""},{"location":"ForecastModel/Atmospheric/ALADIN/#aladin","title":"ALADIN","text":"<p>ALADIN (Limited Area Model Adaptation Dynamique INitialisation) is a limited-area version of ARP\u00c8GE. It is a spectral model; its horizontal domain covers only a well-defined area, , so the fields are \u201cbi-periodicised\u201d to be made able to match with a spectral representation. The vertical coordinate is the same as the one of ARPEGE. ALADIN is forced by using coupling files of ARPEGE. Most of the code is common to ALADIN and ARPEGE, but there are specific applications which require different parts of code. ALADIN currently uses the same package of physics as ARPEGE</p>"},{"location":"ForecastModel/Atmospheric/ALADIN_Dust/","title":"Forecast Model","text":""},{"location":"ForecastModel/Atmospheric/ALADIN_Dust/#aladin-dust","title":"ALADIN Dust","text":"<p>The emissions of desert dust are managed in SURFEX through the DEAD (Dust Entrainment and Deposition) module developed by Zender et al. (2003) and implemented in SURFEX by Grini et al. (2005). This module was improved by Mokhtari et al. (2011). Both versions of this module (original and modified) have been integrated into version 7 of SURFEX. The sequence of physical processes related to desert aerosols is as follows: calculation of the optical properties of desert aerosols, calculation of dry deposition, calculation of turbulent fluxes (vertical diffusion), and calculation of wet deposition.</p> <p></p>"},{"location":"ForecastModel/Atmospheric/AROME/","title":"Forecast Model","text":""},{"location":"ForecastModel/Atmospheric/AROME/#arome","title":"AROME","text":"<p>The convective-scale model AROME (Seity et al., 2011) has been operational at ONM (Algeria) since April 2014, covering the northern part of the country (Latitude: 28\u00b0N - 40\u00b0N, Longitude: 3\u00b0W \u2013 9\u00b0E). AROME takes most of the ALADIN code concerning the adiabatic part of the code (in particular the non-hydrostatic code), the main difference with the current version of ALADIN being the physics package. AROME uses a new physics package well adapted for small mesh-sizes around 2 km, this new physics package is mainly an adaptation of the one which is currently used in the research non-hydrostatic model MESO-NH (used by the team CNRM/GMME for research applications).</p> <p></p>"},{"location":"ForecastModel/Atmospheric/AROME_Dust/","title":"Forecast Model","text":""},{"location":"ForecastModel/Atmospheric/AROME_Dust/#arome-dust","title":"AROME Dust","text":"<p>The model can be configured to use near-real-time aerosols from CAMS. This is done by setting <code>USEAERO=camsnrt</code> , which leads to retrieval of boundary files containing aerosol mass mixing ratio fields from CAMS. Other values of <code>USEAERO</code> are related to use and generation of climatological (2D) aerosol.</p> <p></p>"},{"location":"ForecastModel/Atmospheric/ARPEGE/","title":"Forecast Model","text":""},{"location":"ForecastModel/Atmospheric/ARPEGE/#arpege","title":"ARPEGE","text":"<p>ARPEGE (Action de Recherche Petite Echelle Grande Echelle). This model is used both at METEO-FRANCE and ECMWF; at ECMWF it is named IFS (Integrated Forecasting system). ARPEGE is a global spectral model, with a Gaussian grid for the grid-point calculations. ARPEGE/IFS can work with different physics packages; there is one physics package used at METEO-FRANCE and one other at ECMWF. ARPEGE can be used for climate applications and in this case it uses a slightly different physics package. It is planned to harmonize the physical packages as far as possible between the operational version of ARPEGE (used for routine forecasts) and the climate version of ARPEGE.</p> <p></p>"},{"location":"ForecastModel/Marine/ww3/","title":"Forecast Model","text":""},{"location":"ForecastModel/Marine/ww3/#wave-watch-iii","title":"Wave Watch III","text":"<p>WAVEWATCH III is a numerical model that calculates the evolution of wave action. It is based on a spectral (Fourier) decomposition of the sea state . This software is developed by an international team around NOAA/NCEP. This code computes the evolution of waves in space and time and has been applied at all scales from the global ocean to the beach.</p>"},{"location":"Model_Configuration/Operational_Configuration/ALADIN/","title":"ALADIN Operational Configuration","text":"Cycle Cy43 Latitude 18.5 - 46.5 N Longitude 11W - 17E Horizontal Resolution 18 Km Grid Points 350 * 350 Vertical Resolution 70 Levels Grid Points 72 Hours Coupling Range 1 hour"},{"location":"Model_Configuration/Operational_Configuration/ALADIN_Dust/","title":"ALADIN Dust Operational Configuration","text":"Cycle Cy43 Latitude 18.5 - 46.5 N Longitude 11W - 17E Horizontal Resolution 14 Km Grid Points 250 * 250 Vertical Resolution 70 Levels Grid Points 72 Hours Coupling Range 1 hour"},{"location":"Model_Configuration/Operational_Configuration/AROME/","title":"AROME Operational Configuration","text":"Cycle Cy43 Latitude 28 - 40 N Longitude 3W - 9E Horizontal Resolution 3 Km Grid Points 400 * 400 Vertical Resolution 60 Levels Grid Points 48 Hours Coupling Range 1 hour"},{"location":"Model_Configuration/Research_Configuration/AROME_Dust/","title":"AROME Dust Research &amp; Developement Configuration","text":"Cycle Cy46 Latitude 22 - 33 N Longitude 4.5W - 11E Horizontal Resolution 2.5 Km Grid Points 621 * 521 Vertical Resolution 41 Levels Grid Points 48 Hours Coupling Range 1 hour"},{"location":"NWP_Components/Code_Arch/code_arch/","title":"This Section is Under Construction \ud83d\udea7","text":"<p>We are currently working on this section. Please check back later for updates!</p> <p></p> <p>Thank you for your patience.</p>"},{"location":"NWP_Components/DataAssimilation/Screening/","title":"Screening","text":""},{"location":"NWP_Components/DataAssimilation/Screening/#introduction","title":"Introduction","text":"<p>Screening (configuration 002 of ARPEGE/IFS model) carries out quality control of observations. </p> <p>A useful presentation (Martin Ridal) from the \"Hirlam-B Training Week on HARMONIE system\" training course is available here. Most of the information on this page is based on his presentation.</p>"},{"location":"NWP_Components/DataAssimilation/Surface/CANARI/","title":"Surface Data Assimilation Scheme: Canari","text":""},{"location":"NWP_Components/DataAssimilation/Surface/CANARI/#introduction","title":"Introduction","text":"<p>(by Alena.Trojakova)</p> <p>CANARI stands for Code for the Analysis Necessary for ARPEGE for its Rejects and its Initialization. It is software (part of IFS/ARPEGE source code) to produce an ARPEGE/ALADIN analysis based on optimum interpolation method. The number of ARPEGE/ALADIN configuration is 701. CANARI has the two main components the quality control and an analysis. According to the type of observations the analysis can be: * 3D multivariate for U, V, T, Ps * 3D univariate for RH * 2D univariate for 2m/10m fields * soil parameters analysis is based on 2m increments</p> <p>CANARI can handle following 10 types of observations: * SYNOP: Ps, T2m, RH2m, 10m Wind, RR, Snow depth, SST * AIREP: P ( or Z), Wind, T * SATOB: P, Wind, T - from geostationary satellite imagery * DRIBU: Ps, T2m, 10m Wind, SST * TEMP: P, Wind, T, Q * PILOT: Wind with the corresponding Z, (sometimes 10m Wind) * SATEM: Q, T retrieved from radiances- surface</p>"},{"location":"NWP_Components/Dynamics/dyn/","title":"This Section is Under Construction \ud83d\udea7","text":"<p>We are currently working on this section. Please check back later for updates!</p> <p></p> <p>Thank you for your patience.</p>"},{"location":"NWP_Components/Formats/FileFormats/","title":"File formats in HARMONIE","text":""},{"location":"NWP_Components/Formats/FileFormats/#introduction","title":"Introduction","text":"<p>The HARMONIE system reads and writes a number of different formats. </p>"},{"location":"NWP_Components/Formats/FileFormats/#fa-files","title":"FA files","text":"<p>Default internal format input/output for gridpoint, spectral and SURFEX data. GRIB is used as a way to pack data, but the grib record cannot be used as such.</p> <ul> <li>The header contains information about model domain, projection, spectral truncation, extension zone, boundary zone, vertical levels. </li> <li>Only one date/time per file.</li> <li>FA routines are found under <code>ifsaux/fa</code></li> <li>List or convert a file with Gl</li> <li>Other listing tool PINUTS</li> </ul> <p>Read more</p>"},{"location":"NWP_Components/Formats/FileFormats/#gribgrib2","title":"GRIB/GRIB2","text":"<p>All FA files may be converted to GRIB after the forecast run. For the conversion between FA names and GRIB parameters check this link.</p> <ul> <li>List or convert a GRIB file with Gl</li> </ul>"},{"location":"NWP_Components/Numerical/num/","title":"This Section is Under Construction \ud83d\udea7","text":"<p>We are currently working on this section. Please check back later for updates!</p> <p></p> <p>Thank you for your patience.</p>"},{"location":"NWP_Components/Physics/Diagnostics/Visib/","title":"EPyGrAM","text":""},{"location":"NWP_Components/Physics/Diagnostics/Visib/#general","title":"General","text":"<ul> <li>EPyGram wiki</li> <li>EPyGram doc</li> </ul>"},{"location":"NWP_Components/Physics/Diagnostics/diag1/","title":"This Section is Under Construction \ud83d\udea7","text":"<p>We are currently working on this section. Please check back later for updates!</p> <p></p> <p>Thank you for your patience.</p>"},{"location":"NWP_Components/Physics/Diagnostics/diag2/","title":"This Section is Under Construction \ud83d\udea7","text":"<p>We are currently working on this section. Please check back later for updates!</p> <p></p> <p>Thank you for your patience.</p>"},{"location":"NWP_Components/Physics/Diagnostics/diag3/","title":"This Section is Under Construction \ud83d\udea7","text":"<p>We are currently working on this section. Please check back later for updates!</p> <p></p> <p>Thank you for your patience.</p>"},{"location":"NWP_Components/Physics/Dust/Dust/","title":"This Section is Under Construction \ud83d\udea7","text":"<p>We are currently working on this section. Please check back later for updates!</p> <p></p> <p>Thank you for your patience.</p>"},{"location":"NWP_Components/Physics/Radiation/Ecrad/","title":"This Section is Under Construction \ud83d\udea7","text":"<p>We are currently working on this section. Please check back later for updates!</p> <p></p> <p>Thank you for your patience.</p>"},{"location":"NWP_Components/Physics/Surface/SurFex/","title":"This Section is Under Construction \ud83d\udea7","text":"<p>We are currently working on this section. Please check back later for updates!</p> <p></p> <p>Thank you for your patience.</p>"},{"location":"NWP_Components/Physics/Turb/turb/","title":"This Section is Under Construction \ud83d\udea7","text":"<p>We are currently working on this section. Please check back later for updates!</p> <p></p> <p>Thank you for your patience.</p>"},{"location":"Overview/content/","title":"Overview","text":"<p>The ONM-NWP Wiki Page serves as a comprehensive documentation hub for the Algerian Numerical Weather Prediction (NWP) team. It includes detailed guides and resources organized into various sections, each dedicated to specific aspects of the NWP process. The \"Getting Started\" section offers an overview to introduce users to the platform. The \"Forecast Model\" section covers atmospheric and marine modeling, including ALADIN, AROME, and WaveWatch III models, as well as specialized configurations like dust modeling.</p> <p>The \"NWP Components\" section dives into essential processes such as data assimilation, physical processes like surface interactions and diagnostics (e.g., visibility), and specific dynamics and numerical methods. For detailed model setup, the \"Model Configuration\" section distinguishes between operational and research configurations. Additionally, a \"Control &amp; Verification\" section is included for validation processes like HARP, and \"Seasonal Forecast\" explores long-term weather predictions.</p> <p>Tools used by the NWP team, such as RESTOR, Pinuts, ClimMake, and visualization tools like EPyGram and GrADS, are highlighted in the \"Tools\" section. The page also includes \"Cycle Update\" for tracking model cycle changes, as well as \"About Us\" and \"Contact\" pages for team information. Designed with a clean, user-friendly theme that supports both light and dark modes, the wiki also features search functionality for easy navigation.</p>"},{"location":"Seasonal_Forecast/seasonalforecast/","title":"Seasonal Forecast","text":""},{"location":"Seasonal_Forecast/seasonalforecast/#introduction","title":"Introduction","text":"<p>The ARPEGE model is a global spectral general circulation model developed by M\u00e9t\u00e9o-France in collaboration with the ECMWF for numerical weather prediction. Its climate version, ARPEGE-Climat, was introduced in the 1990s (D\u00e9qu\u00e9 et al., 1994). Over time, it has become the atmospheric component of the CNRM's Earth system model, integrating the atmosphere, ocean, vegetation, and sea ice (Salas y Melia et al., 2005). ARPEGE's grid can be adjusted to shift the pole position and zoom into regions of interest, enabling regional climate studies (D\u00e9qu\u00e9 and Piedelievre, 1995; Gibelin and D\u00e9qu\u00e9, 2003). For instance, the \"Medias\" version of ARPEGE-Climat v4 focuses on the Europe-Mediterranean-North Africa region, with a pole in the Tyrrhenian Sea, a stretching factor of 2.5, and a resolution of about 50 km.</p>"},{"location":"Seasonal_Forecast/seasonalforecast/#operational-machines-used","title":"Operational Machines Used:","text":"<ol> <li>passerellemf (IP: passerellemf.meteo.dz): This serves as a gateway between users (machines) of the PNT and the systems of M\u00e9t\u00e9o-France.</li> <li>Belenos: A supercomputer at M\u00e9t\u00e9o-France where outputs from the Arp\u00e8ge-Climat model are stored.</li> <li>fennec (ONM's supercomputer) (IP: login2.fennec.meteo.dz): This machine runs models such as ALADIN, AROME, ALADIN DUST, ALADIN CLIMAT, and ARPEGE CLIMAT, along with programs that prepare products for clients.</li> <li>Local machine: A virtual machine created for analysis, processing, and visualization of Arp\u00e8ge-Climat outputs. It is equipped with numerous Python packages useful for mathematics, statistics, and data science.</li> </ol>"},{"location":"Seasonal_Forecast/seasonalforecast/#arpege-climat-numerical-forecast-chain","title":"ARPEGE CLIMAT Numerical Forecast Chain","text":"<p>Step 1: Preparation of Files on Belenos</p> <p>To connect to the M\u00e9t\u00e9o-France supercomputer, Belenos, we use the <code>passerellemf</code> machine, which is the only gateway authorized by M\u00e9t\u00e9o-France for server access.</p> <p>Scripts to Execute on passerellemf to Access Belenos:</p> <pre><code> ${HOME}/Belenos\n</code></pre> <p>File Information:</p> <p>Currently, we utilize five meteorological parameters for the seasonal forecast bulletin. To prepare the required outputs for this bulletin, we need to retrieve the first member of the Hindcast (1993-2016), which serves as a climatological reference for calculating anomalies, along with the 50 members of the Forecast to assess probabilities.</p> <p>Example of File Names to Retrieve for the Base Month of September 2020: </p> Hindcast (First member of 1993) Forecast (the 51 member) Parameters H1993I001_arpsfx_6hourly_tas_1993-1994.nc F2020I051_arpsfx_6hourly_tas_2020-2021.nc Average temperature H1993I001_arpsfx_6hourly_tasmax_1993-1994.nc F2020I051_arpsfx_6hourly_tasmax_2020-2021.nc Maximum temperature H1993I001_arpsfx_6hourly_tasmin_1993-1994.nc F2020I051_arpsfx_6hourly_tasmin_2020-2021.nc Minimum temperature H1993I001_arpsfx_6hourly_lwepr_1993-1994.nc F2020I051_arpsfx_6hourly_lwepr_2020-2021.nc Precipitation H1993I001_arpsfx_6hourly_psl_1993-1994.nc F2020I051_arpsfx_6hourly_psl_2020-2021.nc Sea level pressure <p>File description :</p> H1993I001_arpsfx_6hourly_tas_1993-1994.nc F2020I051_arpsfx_daily_lwepr_2020-2021.nc H: Hindcast, 1993: First year of the hindcast, I: Base month September (09), 001: First member, Arpsfx: ARPEGE-SURFEX, 6hourly: One value every 6 hours (4 per day), tas: Average temperature, 1993-1994: 7 months from September 1993 to March 1994, .nc: NetCDF file format. F: Forecast, 2020: Year of the forecast, I: Base month September (09), 051: 51 member, Arpsfx: ARPEGE-SURFEX, daily: One value per day (1 per day), lwepr: Cumulative precipitation, 2020-2021: 7 months from September 2020 to March 2021, .nc: NetCDF file format. <p>The scripts for preparing the hindcast and forecast files on the Belenos machine, which are located in Mr. Mokhtari's account, are <code>Get_Data_Hindcast.sh</code> and <code>Get_Data_Forecast.sh</code>, respectively. </p> <p>These two scripts can be found at the following paths:</p> <pre><code>- `$HOME/STAGE2018/ALADIN_CLIMAT/GetData/ARPEGE_CLIMAT/Get_Data_Hindcast.sh`\n- `$HOME/STAGE2018/ALADIN_CLIMAT/GetData/ARPEGE_CLIMAT/Get_Data_Forecast.sh`\n</code></pre> <p>Downloading Files from M\u00e9t\u00e9o France:</p> <p>The download of these prepared files from Belenos is done on the passerellemf machine. The script that facilitates this task is <code>Get_Data_ARPEGE.sh</code>.</p> <pre><code>\u25ba $HOME/Get_Data_ARPEGE.sh\n</code></pre> <p>Processing Files and Calculating Anomalies and Probabilities: The processing of ARPEGE-Climat files and the calculation of anomalies are performed on our supercomputer (Fennec machine). The scripts that facilitate this task are: <code>Ghreadnc.sh</code>, <code>Ghreadnc_tas.sh</code>, <code>Ghreadnc_psl.sh</code>, <code>Gfreadnc.sh</code>, <code>Gfreadnc_tas.sh</code>, <code>Gfreadnc_mslp.sh</code>, <code>compile</code>, <code>anomaly.exe</code>, <code>anomaly_mslp.exe</code> and <code>anomaly_tas.exe</code>.</p>"},{"location":"Seasonal_Forecast/seasonalforecast/#processing-hindcast-and-forecast-files","title":"Processing Hindcast and Forecast Files:","text":"<p>For the hindcast:</p> <pre><code>- `$HOME/ALADIN_CLIMAT/ARPEGE/Ghreadnc.sh`\n- `$HOME/ALADIN_CLIMAT/ARPEGE/Ghreadnc_tas.sh`\n- `$HOME/ALADIN_CLIMAT/ARPEGE/Ghreadnc_psl.sh`\n</code></pre> <p>The outputs for minimum and maximum temperatures and precipitation are at a daily frequency and are processed by the <code>Ghreadnc.sh</code> script. In contrast, the temperatures at two meters are at a frequency of 6 hours and are processed by the <code>Ghreadnc_tas.sh</code> script.</p> <p>For the Forecast:</p> <pre><code>- `$HOME/ALADIN_CLIMAT/ARPEGE/Gfreadnc.sh`\n- `$HOME/ALADIN_CLIMAT/ARPEGE/Gfreadnc_tas.sh`\n- `$HOME/ALADIN_CLIMAT/ARPEGE/Gfreadnc_mslp.sh`\n</code></pre> <p>Calculating Anomalies and Probabilities:</p> <pre><code>- `$HOME/ALADIN_CLIMAT/ARPEGE/tools/compile`\n- `$HOME/ALADIN_CLIMAT/ARPEGE/tools/anomaly.exe`\n- `$HOME/ALADIN_CLIMAT/ARPEGE/tools/anomaly_tas.exe`\n- `$HOME/ALADIN_CLIMAT/ARPEGE/tools/anomaly_mslp.exe`\n</code></pre> <p>Plotting Maps with Python:</p> <p>The map plotting is done using python3 . The script used for this task is:</p> <pre><code>\u25ba Carte_Oper.sh \u2192 $HOME/ARPEGE_CLIMAT\n</code></pre> <p>After executing this script, the output files in PNG format are generated and stored in a folder named as follows: Carte_ARPEGE_CLIMAT_BM09_2020 (where BM09_2020 refers to the base month of September 2020). These files can be found at the following path:  </p> <pre><code>$HOME/ARPEGE_CLIMAT/cartes\n</code></pre> <p>Plot example</p> <p></p> <p>For more detailed informations contact Walid Chikhi to give you the manuel of the seasonal forecast.</p>"},{"location":"Tools/CliMake/climake/","title":"CliMake","text":""},{"location":"Tools/CliMake/climake/#climake","title":"CliMake","text":"<p>The goal of this package is to provide a simple, flexible and efficient set of files and scripts  in order to make PGD and clim files in various domains. It's a modernization of Fran\u00e7oise Taillefer base scripts, using the GENV/GGET GCO tools. </p> <p>Credits to: Suzana Panezics, Alexandre Mary, Florian Suzat</p> <p>If you have any problem, question or proposition please contact florian.suzat@meteo.fr  </p> <p>To use this script, you need access to MF machine then please follow the following steps:</p> <p>_ clone the CLIMAKE directory. From supercomputer 'beaufix', go to the directory where you want CLIMAKE  script system launch the command:</p> <pre><code>'~suzat/SAVE/cloneClimake'\n</code></pre> <p>This directory contains:  * this README file  * a directory 'geometries' containing only the geometry related namelist blocks for each domain   (both for PGD and 923).   You can create your own file there, but also you can send us those files so that we can archive it.   * a directory 'namelists' containing the PGD and 923 namelists files. Some examples are given.  Geometry related information have been removed from thoses files.  * a directory 'scripts' containing the various scripts files needed for this configuration. Normally, you should not need to edit them, but if you know what you are doing, you can! There are ordered in 4 steps, as they are named.   * the 'climake' main script that take arguments, make some controls and launch the process.   * the 'postpromake' script that works like 'climake', to do lat/lon postprocessing clim files</p> <p>GENV (GCO) CYCLE:</p> <p>First of all, you have to choose what we call a GENV cycle. It's a tag corresponding to a version of the set of files needed for the PGD/923 configuration. To know the last cycle ( the more recent ) you can launch on supercomputer the command  ```bash '/home/mf/dp/marp/martinezs/public/bin/genv oper-clim'  ---&gt; corresponding to the operational suite</p> <pre><code>For cy43 the official GENV cycle is 'cy43t2_clim-op2.02'\n\nThis command will give you a list of key/value.\nThe CYCLE can be read after the key 'CYCLE_NAME'.\nIt contains all the resources useful for the clim configuration (input files, namelists, binaries, etc...).\n\nFor instance, to get the HWSD_SAND database associated to the GENV cycle cy43t2_clim-op1.01, just call:\n```bash\n'/home/mf/dp/marp/martinezs/public/bin/gget hwsd.sand_v2.01.tgz'\n</code></pre> <p>to get the ECOCLIMAP_COVERS_PARAM call:</p> <pre><code>'/home/mf/dp/marp/martinezs/public/bin/gget ecoclimap.covers.param.05.tgz'\n</code></pre> <p>etc... This mecanism avoid to set in the script hard coded links to files, and historisation  (because GENV/GGET check the checksums of files, and ensure they will not be modified)</p> <p>If you don't know what CYCLE to choose, you can use the last operational suite given by </p> <pre><code>'/home/mf/dp/marp/martinezs/public/bin/genv oper-clim'\n</code></pre> <p>here is a list of old genv cycles related to clims... 2017/03/09  cy42_clim-op2.01  2016/11/08  al41t1_clim-op2.02 2016/11/08  al42_clim-op2.01 2016/05/03  al41t1_clim-op2.01 2015/09/23  al41t1_clim-op1.03 2015/08/27  al41t1_clim-op1.01 2015/06/10  al40_clim-op2.04 2015/06/02  al40_clim-op2.03 2015/01/14  al38t1_clim-op3.01 2014/09/29  al40_clim-op1.05 2014/09/22  al40_clim-op1.04 2014/07/31  al40_clim-op1.03 2014/07/09  al40_clim-op1.02 2014/07/09  al38t1_clim-op2.02 2014/04/07  al40_clim-op1.01 2014/04/07  cy40_clim-op1.01 2013/09/16  al38t1_clim-op2.01 to find that you can go to http://gco.meteo.fr/   --&gt; history of cycle put in date field '201*' then update and grep with ctrl +f 'clim'</p> <p>GEOMETRY:</p> <p>They you have to choose 2 geometry namelists (one for the PGD, one for the 923)  either among the existing ones on 'geometries' directory or created by you  (let us know if you want that we archive it).</p> <p>In 923, there is two possibility:   - orography computed in 2 steps  (normal clim configuration)  - orography computed in one step (for example for TELECOM domains)</p> <p>The behaviour for orography steps depends of what you give in arguments on 'climake' command line or config files.</p> <p>LAUNCH</p> <p>since Version 1.21 CLIMAKE can be launched in 2 modes : with config file or interactively * in the first case the command is :</p> <pre><code>    ./climake -c config_file\n</code></pre> <p>you can edit the file config_example.conf to create the file you need * you can either launch CLIMAKE with -i option</p> <pre><code>    ./climake -i\n</code></pre> <p>you will be prompted for the various options to choose...</p> <p>This will use PGD and MASTER binary from the GENV cycle</p> <p>But * You can use your own PGD (resp. MASTER) binary by setting environment variable CUSTOM_PGD (resp. CUSTOM_MASTER) * You can use a custom PGD namelist by replacing the first  'GCO' argument by a path to a namelist (without geometry relateds block). You can start from ./namelist/PGD/GENERIC.nam which is the standard PGD cy43t2 namelist</p> <ul> <li>You can use a custom 923 namelist by replacing the second 'GCO' argument by a path to a namelist (without geometry relateds block). You can start from ./namelist/923/GENERIC.nam which is the standard 923 cy43t2 namelist</li> </ul> <p>Climake will do some check and some pre calculations. You can choose an automatic calculation of NSMAX/NMSMAX from NDGLG and NDLON or manual fixing of thoses values. Before starting the job, climake will produce a sort a short summary, that you have to confirm.</p> <p>RESULTS</p> <p>Return listings of the scripts will arrive into the CLIMAKE directory. You can get the working directory in those return listings (by greping Workdir). You can visit this workdir to show in real time what is happening. In the workdir you will find _all the input files needed _all the listings of each step (called listing.step_XXX) _clim and pgd being constructed Each step in run in independents directory (build_pgd, orography, finalize_orography, const_physiography,...) IMPORTANT: to save space in the HOME directory, the WORKDIR is located in a directory that will be cleaned automatically. If you want to keep it available you must save this $WORKDIR (by tar-gziping it and achiving it on Hendrix for example).</p> <p>Files will finally be produced in the $OUTPUTDIR directory. An extra step producing a html report page, and the plots (every fields of every files) will be produced in this directory.</p> <p>This last step also rewrite the SUFGEOPOTENTIAL field of the clim inside the PGD SFX.ZS field.</p> <p>HAPPY CLIMAKING!</p> <p>POSTPROMAKE </p> <p>Postpromake work as climake. There is 2 interactive mode -f (file mode) and -a (automatic mode) and 1 config file mode -c as climake. -a and -f mode are available with option -c. See examples :</p> <p>for file mode (you need to specify geometry namelist delta)</p> <pre><code>./postpromake -c postpromake_config_filemode_example.conf \n</code></pre> <p>for automatic mode (you need to specify inputs then epygram will calculate for you the geometries namelistsa)</p> <pre><code>./postpromake -c postpromake_config_auto_example.conf \n</code></pre> <p>In automatic mode, postpromake takes as input: _ the resolution in degree of the grid  _ the bounds of the lat lon square</p> <pre><code>             lonmin,latmin (ELON1,ELAT1)                           lonmax,latmin (ELON2,ELAT1)\n                       x --------------------------------------------------- x\n                       |                                                     |\n                       |                                                     |\n                       |                                                     |\n                       |                                                     |\n                       |                                                     |\n                       |                                                     |\n                       x --------------------------------------------------- x\n             lonmin,latmax (ELON1,ELAT2)                           lonmax,latmax (ELON2,ELAT2)\n</code></pre> <p>the program will calculate and generate the geometry files (as GMAP do in Olive) : it uses vortex and epygram, 2 python GMAP toolbox. These calculations are done in the step 1 (after the genv file prefetching) _ then postpromake uses the same scripts as climake. There is only one orography step and no spectral at all (thats why there is a specific namelist delta inside \"stuff\" directory)</p> <p>For the moment epygram is not able to handle lat/lon geometries that have different resolutions in Lat and Lon... It may be available soon....</p>"},{"location":"Tools/Gl/gl/","title":"Gl","text":""},{"location":"Tools/Gl/gl/#introduction","title":"Introduction","text":"<p>gl ( as in griblist ) is a multi purpose tool for file manipulation and conversion. It uses ECMWF's   ecCodes library, and can be compiled with and without support for HARMONIE FA/LFI or NETCDF files.</p> <pre><code> USAGE: gl file [-n namelist_file] [-o output_file] -[lfgmicp(nc)sdtq] [-lbc CONF]\n\n gl [-f] file, list the content of a file, -f for FA/lfi files  \n -c    : Convert a FA/lfi file to grib ( -f implicit )          \n -p    : Convert a FA file to grib output without extension zone\n         (-c and -f implicit )                                  \n -nc   : Convert a FA/lfi file to NetCDF ( -f implicit )        \n -musc : Convert a MUSC FA file ASCII ( -c implicit )           \n -lbc ARG : Convert a CONF file to HARMONIE input               \n            where CONF is ifs or hir as in ECMWF/HIRLAM data    \n         climate_aladin assumed available                       \n -d    : Together with -lbc it gives a (bogus) NH boundary file   \n         climate_aladin assumed available                       \n -s    : Work as silent as possible                             \n -g    : Prints ksec/cadre/lfi info                             \n -m    : Prints min,mean,max of the fields                      \n -i    : Prints the namelist options (useless)                  \n -tp   : Prints the GRIB parameter usage                        \n -t    : Prints the FA/lfi/GRIB table (useful)                  \n -wa   : Prints the atmosphere FA/NETCDF/GRIB table in wiki fmt \n -ws   : Prints the surfex FA/NETCDF/GRIB table in wiki fmt     \n -q    : Cross check the FA/lfi/GRIB table (try)                \n -pl X : Give polster_projlat in degrees                        \n\n gl file -n namelist_file : interpolates file according to      \n                            namelist_file                       \n gl -n namelist_file : creates an empty domain according to     \n                       specifications in namelist_file          \n -igd  : Set lignore_duplicates=T                               \n -igs  : Set lignore_shortname=T. Use indicatorOfParameter      \n             instead of shortName for selection                 \n\n</code></pre>"},{"location":"Tools/Gl/gl/#eccodes-definition-tables","title":"ecCodes definition tables","text":"<p>Since ecCodes has replaced <code>grib_api</code> as the ECMWF primary software package to handle GRIB, we will hereafter only refer to ecCodes but same similar settings applies for <code>grib_api</code> as well. With the change to ecCodes we heavily rely on the shortName key for identification. To get the correct connection between the shortnames and the GRIB1/GRIB2 identifiers we have defined specific tables for harmonie. These tables can be found in <code>/util/gl/definitions</code>. To use these tables you have to define the <code>ECCODES_DEFINITION_PATH</code> environment variable as </p> <pre><code>export ECCODES_DEFINITION_PATH=SOME_PATH/gl/definitions:PATH_TO_YOUR_ECCODES_INSTALLATION\n</code></pre> <p>If this is not set correctly the interpretation of the fields may be wrong.</p>"},{"location":"Tools/Gl/gl/#gribfalfi-file-listing","title":"GRIB/FA/LFI file listing","text":"<p>Listing of GRIB/ASIMOF/FA/LFI files.</p> <pre><code> gl [-l] [-f] [-m] [-g] FILE\n</code></pre> <p>where FILE is in GRIB/ASIMOF/FA/LFI format</p> Option Description <code>-l</code> input format is LFI <code>-f</code> input format is FA <code>-l</code> and <code>-f</code> are equivalent <code>-g</code> print GRIB/FA/LFI header <code>-m</code> print min/mean/max values"},{"location":"Tools/Gl/gl/#gribfalfi-file-conversion","title":"GRIB/FA/LFI file conversion","text":""},{"location":"Tools/Gl/gl/#output-to-grib1","title":"Output to GRIB1","text":"<pre><code>gl [-c] [-p] FILE [ -o OUTPUT_FILE] [ -n NAMELIST_FILE]\n</code></pre> <p>where </p> -c converts the full field (including extension zone) from FA to GRIB1 -p converts field excluding the extension zone (\"p\" as in physical domain) from FA to GRIB1 <p>The FA/LFI to GRIB mapping is done in a table defined by a <code>util/gl/inc/trans_tab.h</code></p> <p>To view the table:</p> <pre><code>gl -t\ngl -tp\n</code></pre> <p>To check for duplicates in the table:</p> <pre><code>gl -q\n</code></pre> <p>The translation from FA/LFI to GRIB1 can be changed through a namelist like this one:</p> <pre><code>  &amp;naminterp\n    user_trans%full_name ='CLSTEMPERATURE',\n    user_trans%t2v       = 253,\n    user_trans%pid       = 123,\n    user_trans%levtype   = 'heigthAboveGround',\n    user_trans%level     = 002,\n    user_trans%tri       = 000,\n  /\n</code></pre> <p>or for the case where the level number is included in the FA name</p> <pre><code>  &amp;naminterp\n    user_trans%full_name='SNNNEZDIAG01',\n    user_trans%cpar='S'\n    user_trans%ctyp='EZDIAG01',\n    ...\n  /\n</code></pre> <p>Conversion can be refined to convert a selection of fields. Below is and example that will write out  * T (shortname='t',pid=011), u (shortname='u',pid=033) andv (shortname='v',pid=034) on all (level=-1) model levels (levtype='hybrid') * T (shortname='t',pid=011) at 2m (lll=2) above the ground (levtype='heightAboveGround') [T2m] * Total precipitation (shortname='tp',pid=061,levtype='heightAboveGround',level=000)</p> <pre><code>  &amp;naminterp\n   readkey%shortname=   't',     'u',     'v',                't',               'tp',               'fg',\n   readkey%levtype='hybrid','hybrid','hybrid','heightAboveGround','heightAboveGround','heightAboveGround',\n   readkey%level=        -1,      -1,      -1,                  2,                  0,                 10,\n   readkey%tri =          0,       0,       0,                  0,                  4,                  2,\n  /\n</code></pre> <p>where  * shortname is the ecCodes shortname of the parameter  * levtype is the ecCodes level type * level is the GRIB level * tri means timeRangeIndicator and is set to distinguish between instantaneous, accumulated and min/max values.</p> <p>The first three ones are well known to most users. The time range indicator is used in ALADIN/AROME to distinguish between instantaneous and accumulated fields. We can also pick variables using their FA/lfi name:</p> <pre><code>  &amp;naminterp\n    readkey%faname = 'SPECSURFGEOP','SNNNTEMPERATURE',\n  /\n</code></pre> <p>Where <code>SNNNTEMPERATURE</code> means that we picks all levels.</p> <p>Fields can be excluded from the conversion by name</p> <pre><code>  &amp;naminterp\n    exclkey%faname = 'SNNNTEMPERATURE'\n  /\n</code></pre>"},{"location":"Tools/Gl/gl/#output-to-grib2","title":"Output to GRIB2","text":"<p>To get GRIB2 files the format has to be set in the namelist as </p> <pre><code>  &amp;naminterp\n    output_format = 'GRIB2'\n  /\n</code></pre> <p>The conversion from FA to GRIB2 is done in gl via the ecCodes tables. All translations are defined in <code>util/gl/scr/harmonie_grib1_2_grib2.pm</code> where we find all settings required to specify a parameter in GRIB1 and GRIB2.</p> <pre><code>\n  tmax =&gt; {\n   editionNumber=&gt; '2',\n   comment=&gt; 'Maximum temperature',\n   discipline=&gt; '0',\n   indicatorOfParameter=&gt; '15',\n   paramId=&gt; '253015',\n   parameterCategory=&gt; '0',\n   parameterNumber=&gt; '0',\n   shortName=&gt; 'tmax',\n   table2Version=&gt; '253',\n   typeOfStatisticalProcessing=&gt; '2',\n   units=&gt; 'K',\n  },\n\n</code></pre> <p>To create ecCodes tables from this file run</p> <pre><code>   cd gl/scr\n   ./gen_tables.pl aladin/arome_grib1_2_grib2\n</code></pre> <p>and copy the grib1/grib2 directories to gl/definitions.</p> <p>Note that there are no GRIB2 transations yet defined for the SURFEX fields!</p>"},{"location":"Tools/Pinuts/pinuts/","title":"PINUTS","text":"<p>In order to use Pinuts just rename it as follow :</p> DOMOLALO Creat a domain giving South,North,East,West and resolution ECTO or ECTPplasme to compute spectral energy on ellipses SUBDO to make ALADIN subdomains MAKDO to make geographical aladin domains EDF or EDitFiles to convert ALADIN/AROME grid-point fields into ASCII files FRODOD to read header and contents of an ALADIN/AROME or FULLPOS file CHAGAL to plot ALADIN/AROME or FULLPOS horizontal fields <p>For more documentation visit Pinuts</p>"},{"location":"Tools/RESTOR/restor/","title":"RESTOR","text":""},{"location":"Tools/RESTOR/restor/#about","title":"About","text":"<p>\"RESTOR\" allows users to perform reanalysis based on \"ALADIN\" and \"AROME\" models for past situations, following user requests (forecasting, climatology, external clients, etc.).</p> <p>This directory is composed of three sub-directories:</p> <ul> <li>ALADIN: For reanalysis using the ALADIN model.</li> <li>AROME: For reanalysis using the AROME model.</li> <li>grads: Contains the necessary scripts for generating maps.</li> </ul> <p>The scripts and programs are designed in a portable format, meaning you can copy the \"RESTOR\" directory into your home directory and run the reanalysis without needing to modify any environment variables.</p>"},{"location":"Tools/RESTOR/restor/#before-the-first-use","title":"Before the First Use","text":"<ul> <li>Insert your access password to the supercomputer in the script \"<code>****_GET_DATA.sh</code>\" at line 34, replacing <code>'HPC_PASSWORD'</code>.</li> <li>Install the <code>sshpass</code> command using: </li> </ul> <pre><code>   apt-get install sshpass\n</code></pre>"},{"location":"Tools/RESTOR/restor/#running-the-processing-scripts","title":"Running the Processing Scripts","text":"<p>To run the scripts, update the date configuration file \"<code>date.config</code>\" with the desired date parameters (year, month, day, etc.).</p> <p>For both models (ALADIN and AROME), there is a main script that automatically executes four (04) sub-scripts:</p>"},{"location":"Tools/RESTOR/restor/#for-aladin","title":"For ALADIN","text":"<p>Run the following command:</p> <pre><code>    ./RUN-RESTOR-ALADIN.sh\n</code></pre> <p>This script executes the following sub-scripts in order: 1. <code>./0-ALAD_GET_DATA.sh</code> 2. <code>./1-ALAD_EDF_JOB.sh</code> 3. <code>./2-ALAD_EXE_JOB.sh</code> 4. <code>./3-ALAD_GRADS_JOB.sh</code></p>"},{"location":"Tools/RESTOR/restor/#for-arome","title":"For AROME","text":"<p>Run the following command:</p> <pre><code>    ./RUN-RESTOR-AROME.sh\n</code></pre> <p>This script executes the following sub-scripts in order: 1. <code>./0-ARO_GET_DATA.sh</code> 2. <code>./1-ARO_EDF_JOB.sh</code> 3. <code>./2-ARO_EXE_JOB.sh</code> 4. <code>./3-ARO_GRADS_JOB.sh</code></p>"},{"location":"Tools/RESTOR/restor/#important","title":"Important","text":"<p>Please check the log files (<code>log.aladin</code> and <code>log.arome</code>) after running the post-processing scripts for each model to ensure that the scripts have executed correctly.</p>"},{"location":"Tools/Visualization/EPyGrAM/","title":"EPyGrAM","text":""},{"location":"Tools/Visualization/EPyGrAM/#general","title":"General","text":"<ul> <li>EPyGram wiki</li> <li>EPyGram doc</li> </ul>"},{"location":"Tools/Visualization/EPyGrAM/#using-epygram-version-152","title":"Using EPyGrAM (version 1.5.2)","text":"<ul> <li>Easy as pie:</li> </ul> <pre><code>  pip install epygram\n</code></pre> <ul> <li>Test:     to plot any parameter using epygram use the following command : epy_cartoplot.py -f [ParamName] [FileName]     Follow the example :</li> </ul> <pre><code>  epy_cartoplot.py -f CLSTEMPERATURE PFAL03ALGE01+0015\n</code></pre> <p>You should get something like this : </p> <p>Enjoy!</p>"},{"location":"Tools/Visualization/grads/","title":"GrADS","text":"<p>The Grid Analysis and Display System (GrADS) is an interactive desktop tool that is used for easy access, manipulation, and visualization of earth science data. GrADS has two data models for handling gridded and station data. GrADS supports many data file formats, including binary (stream or sequential), GRIB (version 1 and 2), NetCDF, HDF (version 4 and 5), and BUFR (for station data).</p> <p>To generate plots with GrADS you need to convert your FA files into a binary file (.dat) using Fortran, then you need to creat a .ctl file which will describe the data inside the converted file.</p> <p>Here's an example of a (ctl) file :</p> <pre><code>DSET dust_surf.dat\nUNDEF  -9999\nTITLE AROME_DUST model\nxdef 621 linear -4.5 0.025\nydef 521 linear  20.0 0.025\nZDEF   1 LEVELS  1000\nTDEF 9 LINEAR 00Z29Jul2024 03hr \nVARS 8\naod   0 99  aerosol optical depth\nflx   0 99  dust flux\nu10   0 99  u-wind\nv10   0 99  v-wind  \nzmsurf   0 99  dust concentration\nt2m   0 99  temperature 2 metres\nssr   0 99  surface solar radiation\nmslp   0 99 MSL Pressure \nENDVARS\n</code></pre> <p>Now we can plot any parameter in this file. If you already have a grads script just run :</p> <pre><code>    grads -blc yourscript.gs\n</code></pre> <p>if you don't have a script just run the following commands :</p> <pre><code>grads\nopen yourctlfile.ctl\nset t [time]\nset grads off\nset poli on\nset grid on\nset gxout shaded\nset mpdset hires\nset map 1 1 6\ncolndx='your level index\ncolevs='your color levels\nset clevs 'colndx'\nset ccols 'colevs'\nd zmsurf*1e-3\ndraw shp alg.shp\nrun cbarn.gs\nset strsiz .17\nset string 1 l 6\ndraw string 0.8 10.8 '  ' Dust concentration (mg/m3) en 03h '\nset strsiz .17\nset string 1 l 6\ndraw string 1.5 10.4 ' '(ech ' chh 'h)'\nset strsiz .16\nset string 1 l 6\ndraw string 5.0 10.5 ' 'Base: ' ana\nset strsiz .16\nset string 1 l 6\ndraw string 5.0 10.2 ' 'Valid: ' fct\nset strsiz .15\nset string 1 l 6\ndraw string 3.0 1.0 ' 'Arome Dust/Algerie (en mg)'\nset display color white\ngxprint con_'it0'.png png x800 y800 white\n</code></pre> <p>the results will be something like this :</p> <p></p>"},{"location":"Useful_links/links/","title":"Useful links","text":""},{"location":"Useful_links/links/#useful-links","title":"Useful links","text":"<ul> <li>Accord</li> <li>Accord wiki</li> <li>Gmapdoc</li> <li>NwpTools</li> <li>IFS documentation</li> <li>Surfex documentation</li> <li>Data assimilation training</li> </ul>"},{"location":"Useful_links/links/#github","title":"Github","text":"<ul> <li>NWP-Dz</li> <li>ACCORD-NWP/IAL (contributing) </li> <li>Hirlam</li> </ul>"}]}